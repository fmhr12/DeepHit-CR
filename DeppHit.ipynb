import pandas as pd
import numpy as np
import torch
from torch import nn
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split
from pycox.models import DeepHit
from pycox.preprocessing.label_transforms import LabTransDiscreteTime
from pycox.evaluation import EvalSurv
import torchtuples as tt
import matplotlib.pyplot as plt
import os
import warnings
from scipy import stats 

warnings.filterwarnings("ignore")

####################################
# 1) Load and Preprocess Data
####################################
file_path = "path_to_your_dataset"
df = pd.read_excel(file_path, sheet_name="Factors")

categorical_vars = [
    'Insurance_Type',
    'Node',
    'Periodontal_Grading',
    'Disease_Site_Merged_2'
]

continuous_vars = [
    'Age',
    'Smoking_Pack_per_Year',
    'Income_1000',
    'Number_Teeth_after_Extraction',
    'RT_Dose'
]

df['time'] = np.clip(df['ClinRad_Time_Indicator_M'].astype(float), 0, 114)
df['event'] = df['ClinRad_M_Competing'].astype(int)
# event coding: 0=censored, 1=event of interest, 2=competing event

# Separate the features and labels
X_cat = df[categorical_vars].astype(str)
X_cont = df[continuous_vars].values.astype(np.float32)
y = (df['time'].values, df['event'].values)

num_risks = int(df['event'].max())  # e.g., 2 if 0->censor,1->event of interest,2->competing
num_durations = 115

# One-hot encoding
ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
ohe.fit(X_cat)
X_cat_ohe = ohe.transform(X_cat)

# Combine categorical + continuous
X_all = np.hstack([X_cat_ohe, X_cont]).astype(np.float32)
in_features = X_all.shape[1]

# For stratification focusing on event=1
y_event1 = (y[1] == 1).astype(int)

####################################
# 2) Define CauseSpecificNet
####################################
class CauseSpecificNet(nn.Module):
    def __init__(self, in_features, num_nodes_shared, num_nodes_indiv, num_risks,
                 out_features, batch_norm=True, dropout=0.1):
        super().__init__()
        # Shared part
        self.shared_net = tt.practical.MLPVanilla(
            in_features, 
            num_nodes_shared[:-1],   # hidden layers (except last)
            num_nodes_shared[-1],    # last hidden layer size
            batch_norm=batch_norm, dropout=dropout
        )
        # Risk-specific part
        self.risk_nets = nn.ModuleList()
        for _ in range(num_risks):
            net = tt.practical.MLPVanilla(
                num_nodes_shared[-1], # input to risk net is output of shared
                num_nodes_indiv,
                out_features,
                batch_norm=batch_norm,
                dropout=dropout,
            )
            self.risk_nets.append(net)

    def forward(self, input):
        out = self.shared_net(input)
        out = [net(out) for net in self.risk_nets]
        out = torch.stack(out, dim=1)
        return out

# Example sizes
num_nodes_shared = [50, 30]
num_nodes_indiv = [30]

####################################
# 3) Hyperparameter Grid (example)
####################################
param_grid = {
    'lr': [0.001],
    'alpha': [0.2],
    'sigma': [0.1],
    'dropout': [0.5],
    'batch_size': [128]
}

####################################
# 4) Repeated Stratified (5×5) CV
####################################
# This yields 5 folds × 5 repeats = 25 distinct splits
rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=123)

# Prepare output
output_dir = "deep_hit_predictions"
os.makedirs(output_dir, exist_ok=True)

fold_c_indexes = []
fold_ibss = []  # Initialize list to store IBS for each fold

for fold_num, (train_idx, test_idx) in enumerate(rskf.split(X_all, y_event1)):
    # fold_num goes from 0 to 24
    print("======================================================")
    print(f"   Training fold {fold_num + 1} of 25 (Repeated CV)")
    print("======================================================")
    
    # Outer Train/Test split
    X_train_full, X_test = X_all[train_idx], X_all[test_idx]
    y_train_full = (y[0][train_idx], y[1][train_idx])
    y_test_fold = (y[0][test_idx], y[1][test_idx])
    
    # Simple "inner" train/val split (not a full cross-val, but one split)
    tr_idx, val_idx = train_test_split(
        np.arange(len(X_train_full)), 
        test_size=0.2,
        stratify=y_train_full[1],
        random_state=fold_num  # changes each fold
    )
    
    X_train, X_val = X_train_full[tr_idx], X_train_full[val_idx]
    y_train = (y_train_full[0][tr_idx], y_train_full[1][tr_idx])
    y_val   = (y_train_full[0][val_idx], y_train_full[1][val_idx])
    
    # Scale continuous features only (keep OHE columns untouched)
    cat_dim = X_cat_ohe.shape[1]
    scaler = StandardScaler()
    X_train[:, cat_dim:] = scaler.fit_transform(X_train[:, cat_dim:])
    X_val[:, cat_dim:]   = scaler.transform(X_val[:, cat_dim:])
    X_test[:, cat_dim:]  = scaler.transform(X_test[:, cat_dim:])
    
    # Discretize durations
    labtrans = LabTransDiscreteTime(num_durations)
    durations_train, events_train = labtrans.fit_transform(*y_train)
    durations_val, events_val = labtrans.transform(*y_val)
    
    durations_train = durations_train.astype(np.int64)
    events_train    = events_train.astype(np.int64)
    durations_val   = durations_val.astype(np.int64)
    events_val      = events_val.astype(np.int64)
    
    durations_test_fold, events_test_fold = y_test_fold
    
    ###################################
    # Hyperparameter Tuning on val set
    ###################################
    best_val_c_index = -np.inf
    best_params      = None
    best_log         = None
    best_model       = None
    
    for lr in param_grid['lr']:
        for alpha in param_grid['alpha']:
            for sigma in param_grid['sigma']:
                for dropout in param_grid['dropout']:
                    for batch_size in param_grid['batch_size']:
                        out_features = labtrans.out_features
                        
                        # Build the network
                        net = CauseSpecificNet(
                            in_features, 
                            num_nodes_shared,
                            num_nodes_indiv,
                            num_risks,
                            out_features,
                            batch_norm=True,
                            dropout=dropout
                        )
                        
                        model = DeepHit(net, tt.optim.Adam(lr=lr),
                                        alpha=alpha, sigma=sigma,
                                        duration_index=labtrans.cuts)
                        
                        # Training
                        val_data = (X_val, (durations_val, events_val))
                        log = model.fit(
                            X_train, (durations_train, events_train),
                            batch_size=batch_size,
                            epochs=200,
                            val_data=val_data,
                            verbose=False
                        )
                        
                        # Evaluate on validation set (for event=1)
                        cif_val = model.predict_cif(X_val)  # list of CIF arrays per event
                        cif1_val = pd.DataFrame(cif_val[0], index=model.duration_index)
                        
                        ev_val = EvalSurv(
                            1 - cif1_val,
                            y_val[0],
                            (y_val[1] == 1),
                            censor_surv='km'
                        )
                        c_index_val = ev_val.concordance_td()
                        
                        if c_index_val > best_val_c_index:
                            best_val_c_index = c_index_val
                            best_params = (lr, alpha, sigma, dropout, batch_size)
                            best_log = log
                            best_model = model
    
    # Report best hyperparameters
    print(f"Best Params for fold {fold_num+1}: "
          f"LR={best_params[0]}, alpha={best_params[1]}, sigma={best_params[2]}, "
          f"dropout={best_params[3]}, batch_size={best_params[4]}, "
          f"val_cindex={best_val_c_index:.4f}")
    
    # Evaluate on the Test set using best model
    cif_test = best_model.predict_cif(X_test)  # list of shape [num_risks], each [num_times, n_test]
    duration_index = best_model.duration_index
    
    # Build output DataFrame for saving
    df_out = pd.DataFrame({
        'time': durations_test_fold,
        'delta': events_test_fold
    })
    
    # Add columns for the predicted CIF at each discrete time, for each event
    for r in range(num_risks):
        for t_idx, t_val in enumerate(duration_index):
            df_out[f'pred_event{r+1}_time{t_val}'] = cif_test[r][t_idx, :]
    
    # Save CSV file with all predicted CIFs
    csv_file = os.path.join(output_dir, f"fold_{fold_num+1}_predictions.csv")
    df_out.to_csv(csv_file, index=False)
    
    # (Optional) Compute c-index on the test set for demonstration
    cif1_test = pd.DataFrame(cif_test[0], index=duration_index)
    ev_test = EvalSurv(
        1 - cif1_test,
        durations_test_fold,
        (events_test_fold == 1),
        censor_surv='km'
    )
    c_index_test = ev_test.concordance_td()
    fold_c_indexes.append(c_index_test)
    print(f"Fold {fold_num+1} Test C-index for event=1: {c_index_test:.4f}")
    
    ###################################
    # Compute Integrated Brier Score (IBS) Manually
    ###################################
    # Define the time grid using unique event times from the test set
    unique_event_times = np.unique(durations_test_fold[events_test_fold == 1])
    time_grid = unique_event_times  # Alternatively, define a fixed grid
    
    # Compute Brier scores at each time in the time grid
    brier_scores = ev_test.brier_score(time_grid)
    
    # Integrate the Brier scores using the trapezoidal rule
    ibs_test = np.trapz(brier_scores, time_grid) / (time_grid[-1] - time_grid[0])
    fold_ibss.append(ibs_test)
    print(f"Fold {fold_num+1} Test Integrated Brier Score (IBS): {ibs_test:.4f}")

####################################
# 5) Calculate 95% Confidence Intervals for C-index and IBS
####################################

def compute_confidence_interval(data, confidence=0.95):
    """
    Compute the mean and confidence interval for a list of values.
    
    Parameters:
    - data: list or array of numerical values
    - confidence: float, the confidence level (default 0.95)
    
    Returns:
    - mean: float, the mean of the data
    - lower: float, the lower bound of the confidence interval
    - upper: float, the upper bound of the confidence interval
    """
    n = len(data)
    mean = np.mean(data)
    sem = stats.sem(data)
    h = sem * stats.t.ppf((1 + confidence) / 2., n-1)
    return mean, mean - h, mean + h

# Compute confidence intervals for C-index
cindex_mean, cindex_lower, cindex_upper = compute_confidence_interval(fold_c_indexes)

# Compute confidence intervals for IBS
ibs_mean, ibs_lower, ibs_upper = compute_confidence_interval(fold_ibss)

# Report the results
print("\n====================== Summary ======================")
print(f"C-index: {cindex_mean:.4f} (95% CI: {cindex_lower:.4f} - {cindex_upper:.4f})")
print(f"IBS:      {ibs_mean:.4f} (95% CI: {ibs_lower:.4f} - {ibs_upper:.4f})")
print("======================================================")
